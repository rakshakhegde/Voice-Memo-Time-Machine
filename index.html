<!doctype html>
<html lang="en">

<link rel="icon" type="image/x-icon" href="voice_memo_time_machine.ico">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Voice Memo Time Machine</title>
  <style>
    :root {
      --bg: #0b0f12;
      --card: #0f1620;
      --muted: #8b98a6;
      --accent: #6ee7b7;
      --danger: #ff6b6b;
      --glass: rgba(255, 255, 255, 0.03);
    }

    html,
    body {
      height: 100%;
      margin: 0;
      background: var(--bg);
      color: #e6eef6;
      font-family: Inter, ui-sans-serif, system-ui, Segoe UI, Roboto, "Helvetica Neue", Arial;
    }

    .app {
      max-width: 1100px;
      margin: 28px auto;
      padding: 24px;
      display: grid;
      grid-template-columns: 360px 1fr;
      gap: 20px;
    }

    .panel {
      background: linear-gradient(180deg, rgba(255, 255, 255, 0.02), transparent);
      border-radius: 14px;
      padding: 18px;
      box-shadow: 0 6px 30px rgba(0, 0, 0, 0.6);
      border: 1px solid rgba(255, 255, 255, 0.03);
    }

    h1 {
      margin: 0 0 10px 0;
      font-size: 18px
    }

    .muted {
      color: var(--muted);
      font-size: 13px
    }

    select,
    button {
      background: transparent;
      border: 1px solid rgba(255, 255, 255, 0.06);
      padding: 10px;
      border-radius: 10px;
      color: inherit;
      font-size: 14px
    }

    .controls {
      display: flex;
      flex-direction: column;
      gap: 12px
    }

    .row {
      display: flex;
      gap: 10px;
      align-items: center
    }

    .durations {
      display: flex;
      flex-wrap: wrap;
      gap: 8px
    }

    .dur-btn {
      padding: 8px 12px;
      border-radius: 999px;
      background: var(--glass);
      cursor: pointer;
      border: 1px solid rgba(255, 255, 255, 0.02)
    }

    .dur-btn:hover {
      transform: translateY(-2px)
    }

    .status {
      font-size: 12px;
      color: var(--muted)
    }

    .meter {
      height: 10px;
      border-radius: 8px;
      background: linear-gradient(90deg, rgba(255, 255, 255, 0.04), rgba(255, 255, 255, 0.02));
      overflow: hidden
    }

    .level {
      height: 100%;
      width: 0;
      background: linear-gradient(90deg, var(--accent), #2ad1a3)
    }

    canvas {
      width: 100%;
      height: 120px;
      border-radius: 10px;
      background: linear-gradient(180deg, rgba(255, 255, 255, 0.01), rgba(255, 255, 255, 0.00));
      display: block
    }

    .right {
      display: flex;
      flex-direction: column;
      gap: 16px;
    }

    .snippet-card {
      display: flex;
      flex-direction: column;
      gap: 8px;
      padding: 12px;
      background: linear-gradient(180deg, rgba(255, 255, 255, 0.015), transparent);
      border-radius: 12px;
      border: 1px solid rgba(255, 255, 255, 0.02)
    }

    .snippet-controls {
      display: flex;
      gap: 10px;
      align-items: center
    }

    .player {
      display: flex;
      flex: 1;
      gap: 12px;
    }

    audio {
      width: 100%;
      border-radius: 8px;
    }

    .download {
      padding: 8px 12px;
      border-radius: 10px;
      background: transparent;
      border: 1px solid rgba(255, 255, 255, 0.06);
      cursor: pointer
    }

    .small {
      font-size: 13px
    }

    .device-label {
      font-weight: 600
    }

    .hint {
      font-size: 12px;
      color: var(--muted)
    }

    footer {
      color: var(--muted);
      font-size: 12px;
      text-align: center;
      margin-top: 12px
    }

    .warning {
      color: var(--danger);
      font-size: 13px
    }

    @media (max-width:880px) {
      .app {
        grid-template-columns: 1fr;
        padding: 12px
      }
    }

    #liveClock {
      white-space: nowrap;
      text-align: right;
      font-variant-numeric: tabular-nums;
    }
  </style>
</head>

<body>
  <div class="app">
    <div class="panel">
      <h1 style="cursor: pointer" onclick="window.open('voice_memo_time_machine.jpg', '_blank')" style="cursor: pointer"
        onclick="window.open('voice_memo_time_machine.jpg', '_blank')">Rolling Hour Recorder</h1>
      <div class="muted">Records continuously (rolling 1 hour). Extract snippets while recording.</div>
      <div style="height:14px"></div>

      <div class="controls">
        <div>
          <div style="display:flex;justify-content:space-between;align-items:end;">
            <div>
              <div class="device-label" id="deviceLabel">Microphone: â€”</div>
              <div class="hint" style="margin-top:0px;">Choose input microphone (default: Macbook Pro Microphone)</div>
            </div>
            <div style="text-align:right">
              <div class="status" id="recStatus">Status: initializingâ€¦</div>
              <div class="status" id="srDisplay" style="margin-top:2px;">Sample rate: â€”</div>
            </div>
          </div>
          <div style="height:10px"></div>
          <div class="row">
            <select id="deviceSelect"></select>
            <button id="restartBtn">Restart</button>
          </div>
        </div>

        <div>
          <div class="muted small">Live waveform (past 30s)</div>
          <canvas id="liveWave" width="800" height="120"></canvas>
          <div style="height:8px"></div>
          <div class="meter">
            <div id="level" class="level"></div>
          </div>
        </div>

        <div>
          <div class="muted small">Extract snippet durations</div>
          <div class="durations" id="durations">
            <button class="dur-btn" data-sec="10">10s</button>
            <button class="dur-btn" data-sec="30">30s</button> <!-- new -->
            <button class="dur-btn" data-sec="60">1m</button>
            <button class="dur-btn" data-sec="120">2m</button> <!-- new -->
            <button class="dur-btn" data-sec="300">5m</button>
            <button class="dur-btn" data-sec="600">10m</button>
            <button class="dur-btn" data-sec="1800">30m</button>
            <button class="dur-btn" data-sec="3600">1h</button>
          </div>
        </div>

        <div>
          <div class="muted small">Storage</div>
          <div class="hint">Buffer: last 1 hour of raw audio (16-bit PCM) â€” memory usage depends on mic sample rate.
          </div>
          <div style="height:8px"></div>
          <div id="memoryInfo" class="muted small">Est. buffer bytes: â€”</div>
          <div id="logger" class="muted small">Logger</div>
        </div>
      </div>
      <footer>Built â€” Rolling recorder (client-only). No external servers.</footer>
    </div>

    <div class="right">
      <div class="panel">
        <div style="display:flex;justify-content:space-between;align-items:center">
          <div>
            <div style="font-weight:700">Snippets</div>
            <div class="muted small" style="margin-top:4px;">Choose a duration, or click any existing snippet to inspect
            </div>
          </div>
          <div id="liveClock" class="muted small">â€”</div>
        </div>

        <div id="snippetsArea" style="margin-top:12px;display:flex;flex-direction:column;gap:12px">
          <div id="no-snippet-msg" class="muted small">No snippet selected yet. Click a duration to extract.</div>
        </div>
      </div>

      <div class="panel">
        <div class="muted small">Notes & tips</div>
        <ul class="muted small">
          <li>Allow microphone access when prompted. The app will start recording automatically after permission.</li>
          <li>Snippets are generated client-side as WAV files and downloadable.</li>
          <li>If the browser prevents autoplaying audio or starting recording on load, click "Restart" to initialize
            after granting permission.</li>
        </ul>
        <div style="height:10px"></div>
        <div id="warnBox" class="warning" style="display:none"></div>
      </div>
    </div>
  </div>

  <script>
    /*
     Rolling Hour Recorder with AudioWorklet (single-file)
     - circular buffer: Int16Array storing mono PCM for last 1 hour
     - inline AudioWorklet module created from a Blob
     - live waveform (30s) and snippet extraction to WAV
    */

    (async function () {
      // UI references
      const deviceSelect = document.getElementById('deviceSelect');
      const deviceLabel = document.getElementById('deviceLabel');
      const srDisplay = document.getElementById('srDisplay');
      const recStatus = document.getElementById('recStatus');
      const liveWave = document.getElementById('liveWave');
      const liveCtx = liveWave.getContext('2d');
      const levelBar = document.getElementById('level');
      const durationsDiv = document.getElementById('durations');
      const snippetsArea = document.getElementById('snippetsArea');
      const memoryInfo = document.getElementById('memoryInfo');
      const restartBtn = document.getElementById('restartBtn');
      const liveClock = document.getElementById('liveClock');
      const warnBox = document.getElementById('warnBox');
      const loggerDiv = document.getElementById('logger');

      // Click to clear
      loggerDiv.addEventListener('click', () => {
        loggerDiv.innerHTML = 'Logger'; // reset text
      });

      // config
      const MAX_SECONDS = 3600; // 1 hour
      const LIVE_SECONDS = 30;  // live waveform window
      let audioContext = null;
      let micStream = null;
      let sourceNode = null;
      let sampleRate = 48000;
      let buffer = null; // Int16Array circular buffer
      let bufferLength = 0;
      let writeIndex = 0;
      let recording = false;
      let channelCount = 1;
      let currentVolume = 0;
      let selectedDeviceId = null;
      let deviceList = [];
      let memoryBytesEstimate = 0;
      let lastClickedSnippet = null;
      let allPlayers = [];
      let reuseBuffer = null;

      let lastUsedMicLabel = localStorage.getItem('lastUsedMicLabel') || 'MacBook Pro Microphone';
      let lastUsedDeviceId = localStorage.getItem('lastUsedDeviceId');

      // Inline AudioWorklet code
      const workletCode = `
        class RecorderProcessor extends AudioWorkletProcessor {
          constructor() {
            super();
            this.buffer = new Float32Array(128); // typical AudioWorklet block size
          }
          process(inputs) {
            const input = inputs[0];
            if (input && input.length > 0) {
              const ch0 = input[0];
              this.buffer.set(ch0);              // copy into pre-allocated buffer
              this.port.postMessage(this.buffer); // no transfer
            }
            return true;
          }
        }
        registerProcessor('recorder-processor', RecorderProcessor);
      `;
      const workletBlob = new Blob([workletCode], { type: 'application/javascript' });
      const workletURL = URL.createObjectURL(workletBlob);

      let recorderNode;

      // Start initialization
      log('await initAndStart start');
      await initAndStart();
      log('await initAndStart done');

      // UI wiring
      log('restart UI wiring');
      restartBtn.onclick = async () => { await restart(); };
      deviceSelect.onchange = async () => { selectedDeviceId = deviceSelect.value; await restart(); };
      durationsDiv.addEventListener('click', async ev => {
        const btn = ev.target.closest('.dur-btn');
        if (!btn) return;
        const seconds = Number(btn.dataset.sec);
        await createSnippet(seconds);
      });

      // ---------- Initialization ----------
      async function initAndStart() {
        log('initAndStart');
        try {
          //await ensureAudioContext();
          //await getDevicesAndPopulate();
          await startRecordingWithWorklet();
          recStatus.textContent = 'Status: recording';
          recording = true;
        } catch (err) {
          console.error('init error', err);
          recStatus.textContent = 'Status: stopped';
          warnBox.style.display = 'block';
          warnBox.textContent = 'Microphone access required â€” click "Restart" and allow microphone. Some browsers prevent auto-start on load.';
        }
        requestAnimationFrame(drawLiveWave);
        setInterval(() => { liveClock.textContent = new Date().toLocaleString(); }, 1000);
      }

      async function restart() {
        log('restart');
        await stopRecording();
        //await ensureAudioContext();
        //await getDevicesAndPopulate();
        await startRecordingWithWorklet();
      }

      async function ensureAudioContext() {
        log('ensureAudioContext');
        if (audioContext && audioContext.state !== 'closed') return;
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        sampleRate = audioContext.sampleRate;
        srDisplay.textContent = 'Sample rate: ' + sampleRate + ' Hz';
        allocateCircularBuffer();
      }

      function allocateCircularBuffer() {
        log('allocateCircularBuffer');
        if (sampleRate * MAX_SECONDS == bufferLength) {
          log('Reusing existing circular buffer');
          return;
        }
        bufferLength = sampleRate * MAX_SECONDS;
        buffer = new Int16Array(bufferLength);
        writeIndex = 0;
        memoryBytesEstimate = bufferLength * 2;
        memoryInfo.textContent = 'Est. buffer bytes: ' + formatBytes(memoryBytesEstimate) + ' (' + bufferLength + ' samples)';
      }

      function formatBytes(n) {
        if (n > 1e9) return (n / 1e9).toFixed(2) + ' GB';
        if (n > 1e6) return (n / 1e6).toFixed(2) + ' MB';
        if (n > 1e3) return (n / 1e3).toFixed(2) + ' KB';
        return n + ' bytes';
      }

      async function getDevicesAndPopulate2() {
        log('getDevicesAndPopulate');
        try {
          if (!micStream) {
            // Use saved deviceId if available
            log('lastUsedDeviceId', lastUsedDeviceId);
            const constraints = {
              audio: lastUsedDeviceId ?
                { deviceId: { exact: lastUsedDeviceId } } :
                true
            };
            micStream = await navigator.mediaDevices.getUserMedia(constraints);

            // Get actual sample rate from the track settings
            const audioTrack = micStream.getAudioTracks()[0];
            if (audioTrack) {
              log('Microphone Device:', audioTrack.label);
              log('TrackSettings:', JSON.stringify(audioTrack.getSettings(), null, 2));
              const settings = audioTrack.getSettings();
              const audioTrackSampleRate = settings.sampleRate;
              log('getDevicesAndPopulate Sample rate: ' + audioTrackSampleRate);
            }
          }
        } catch (e) {
          // ignore â€” user may have blocked; still try to enumerate
        }

        const devices = await navigator.mediaDevices.enumerateDevices();
        deviceList = devices.filter(d => d.kind === 'audioinput');
        log('device list:', deviceList);

        // Find default index based on saved label or deviceId
        let defaultIndex = 0;
        for (let i = 0; i < deviceList.length; i++) {
          const device = deviceList[i];
          if (device.deviceId === lastUsedDeviceId || device.label === lastUsedMicLabel) {
            defaultIndex = i;
            log(device);
            break;
          }
        }

        deviceSelect.innerHTML = '';
        deviceList.forEach((d, i) => {
          const opt = document.createElement('option');
          opt.value = d.deviceId;
          opt.textContent = d.label || ('Microphone ' + (i + 1));
          deviceSelect.appendChild(opt);
        });

        if (deviceList.length === 0) {
          deviceSelect.innerHTML = '<option value="">(no input devices found)</option>';
          deviceLabel.textContent = 'Microphone: (none)';
          return;
        }

        let chosenDeviceId = deviceList[defaultIndex].deviceId;
        if (selectedDeviceId) chosenDeviceId = selectedDeviceId;
        deviceSelect.value = chosenDeviceId;
        selectedDeviceId = chosenDeviceId;

        const selDevice = deviceList.find(d => d.deviceId === chosenDeviceId);
        if (selDevice) {
          log('Selected device:', selDevice);
          // Save the selected device info
          lastUsedMicLabel = selDevice.label;
          lastUsedDeviceId = selDevice.deviceId;
          localStorage.setItem('lastUsedMicLabel', lastUsedMicLabel);
          localStorage.setItem('lastUsedDeviceId', lastUsedDeviceId);

          deviceLabel.textContent = 'Microphone: ' + selDevice.label;
        } else {
          deviceLabel.textContent = 'Microphone: Selected input';
        }
      }

      // Populate input devices (try to get labels by requesting permission first)
      async function getDevicesAndPopulate() {
        log("ðŸŽ¤ getDevicesAndPopulate (hybrid, optimized reuse)");

        const savedId = selectedDeviceId || localStorage.getItem("lastUsedDeviceId");
        const savedLabel = localStorage.getItem("lastUsedMicLabel");

        // 1ï¸âƒ£ Initial permission request
        let tempStream = null;
        try {
          if (savedId) {
            tempStream = await navigator.mediaDevices.getUserMedia({
              audio: { deviceId: { exact: savedId } },
            });
          } else {
            tempStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          }
        } catch {
          tempStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        }

        // 2ï¸âƒ£ Now labels are available
        const devices = (await navigator.mediaDevices.enumerateDevices())
          .filter((d) => d.kind === "audioinput");

        // 3ï¸âƒ£ Pick best device (ID â†’ label â†’ MacBook â†’ first)
        let chosenDevice =
          devices.find((d) => d.deviceId === savedId) ||
          devices.find((d) => d.label === savedLabel) ||
          devices.find((d) => d.label.includes("MacBook Pro Microphone")) ||
          devices[0];

        // 4ï¸âƒ£ Update UI
        deviceSelect.innerHTML = "";
        devices.forEach((d) => {
          const opt = document.createElement("option");
          opt.value = d.deviceId;
          opt.textContent = d.label || "Microphone " + (deviceSelect.length + 1);
          deviceSelect.appendChild(opt);
        });

        deviceSelect.value = chosenDevice.deviceId;
        deviceLabel.textContent = "Microphone: " + (chosenDevice.label || "Unknown");

        localStorage.setItem("lastUsedDeviceId", chosenDevice.deviceId);
        localStorage.setItem("lastUsedMicLabel", chosenDevice.label || "");

        // 5ï¸âƒ£ Reuse or reopen stream as needed
        const tempTrack = tempStream.getAudioTracks()[0];
        const tempDeviceId = tempTrack.getSettings().deviceId;

        if (tempDeviceId && tempDeviceId === chosenDevice.deviceId) {
          log("â™»ï¸ Reusing existing tempStream for chosen device:", chosenDevice.label);
          micStream = tempStream;
        } else {
          log("ðŸ”„ Opening new stream for:", chosenDevice.label);
          tempStream.getTracks().forEach((t) => t.stop());
          micStream = await navigator.mediaDevices.getUserMedia({
            audio: { deviceId: { exact: chosenDevice.deviceId } },
          });
        }

        log("âœ… Using mic:", chosenDevice.label);
        return { devices, chosenDevice, micStream };
      }

      // ---------- AudioWorklet recording ----------
      async function startRecordingWithWorklet2() {
        log('old startRecordingWithWorklet');
        if (!audioContext) await ensureAudioContext();

        await getDevicesAndPopulate();

        if (sourceNode) sourceNode.disconnect();
        sourceNode = audioContext.createMediaStreamSource(micStream);

        await audioContext.audioWorklet.addModule(workletURL);
        URL.revokeObjectURL(workletURL);

        const workletNode = new AudioWorkletNode(audioContext, 'recorder-processor', {
          numberOfInputs: 1,
          numberOfOutputs: 0, // we don't output audio from the worklet
          channelCount: 1
        });

        // receive Float32Array chunks from worklet
        workletNode.port.onmessage = (e) => {
          const floatSamples = e.data; // still owned by worklet
          writeToCircularBuffer(floatSamples);
          updateLevelMeter(floatSamples);
        };

        // connect: source -> worklet (no output to destination required)
        sourceNode.connect(workletNode);
        // connect a gain of 0 to destination if some browsers require a connection to destination
        // but we avoid audible output by not connecting workletNode to destination.
        recStatus.textContent = 'Status: recording';
        recording = true;
      }

      async function startRecordingWithWorklet() {
        log("ðŸŽ™ï¸ unified startRecordingWithWorklet");

        // 1ï¸âƒ£ Determine which mic to use
        const savedId = selectedDeviceId || localStorage.getItem("lastUsedDeviceId");
        const savedLabel = localStorage.getItem("lastUsedMicLabel");

        // 2ï¸âƒ£ Step 1: request temp stream (to unlock labels)
        let tempStream = null;
        try {
          if (savedId) {
            tempStream = await navigator.mediaDevices.getUserMedia({
              audio: { deviceId: { exact: savedId } },
            });
          } else {
            tempStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          }
        } catch (err) {
          console.warn("âš ï¸ Could not open saved device, falling back to default mic:", err);
          tempStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        }

        // 3ï¸âƒ£ Step 2: enumerate devices (labels now available)
        const devices = (await navigator.mediaDevices.enumerateDevices())
          .filter((d) => d.kind === "audioinput");

        // 4ï¸âƒ£ Step 3: pick the correct device using hybrid logic
        let chosenDevice =
          devices.find((d) => d.deviceId === savedId) ||
          devices.find((d) => d.label === savedLabel) ||
          devices.find((d) => d.label == 'MacBook Pro Microphone') ||
          devices[0];

        if (!chosenDevice) {
          recStatus.textContent = 'âŒ No input devices available.';
          return;
        }

        log("ðŸŽ§ Chosen mic:", chosenDevice.label, chosenDevice.deviceId);

        // 5ï¸âƒ£ Step 4: update dropdown UI
        deviceSelect.innerHTML = "";
        devices.forEach((d) => {
          const opt = document.createElement("option");
          opt.value = d.deviceId;
          opt.textContent = d.label || `Microphone ${deviceSelect.length + 1}`;
          deviceSelect.appendChild(opt);
        });
        deviceSelect.value = chosenDevice.deviceId;
        deviceLabel.textContent = "Microphone: " + (chosenDevice.label || "Unknown");

        // 6ï¸âƒ£ Step 5: update persistent info
        localStorage.setItem("lastUsedDeviceId", chosenDevice.deviceId);
        localStorage.setItem("lastUsedMicLabel", chosenDevice.label || "");
        selectedDeviceId = chosenDevice.deviceId;

        // 7ï¸âƒ£ Step 6: reuse or reopen mic stream
        const tempTrack = tempStream.getAudioTracks()[0];
        const tempDeviceId = tempTrack.getSettings().deviceId;

        if (tempDeviceId === chosenDevice.deviceId) {
          log("â™»ï¸ Reusing existing tempStream for chosen mic:", chosenDevice.label);
          micStream = tempStream;
        } else {
          log("ðŸ”„ Opening new stream for chosen mic:", chosenDevice.label);
          tempStream.getTracks().forEach((t) => t.stop());
          micStream = await navigator.mediaDevices.getUserMedia({
            audio: { deviceId: { exact: chosenDevice.deviceId } },
          });
        }

        // 8ï¸âƒ£ Step 7: setup AudioContext and worklet
        if (!audioContext || audioContext.state === 'closed') {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
          sampleRate = audioContext.sampleRate;
          srDisplay.textContent = 'Sample rate: ' + sampleRate + ' Hz';
          allocateCircularBuffer();
          audioContext.onstatechange = () => {
            logToDiv(`ðŸŽ§ AudioContext state: ${audioContext.state}`);
          };
        } else if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }

        try { sourceNode?.disconnect(); } catch (e) {
          console.error('sourceNode disconnect error', e);
        }
        sourceNode = audioContext.createMediaStreamSource(micStream);

        if (!audioContext.workletModuleLoaded) {
          await audioContext.audioWorklet.addModule(workletURL);
          audioContext.workletModuleLoaded = true;

          // Optional: only revoke if you re-create workletURL each time
          // URL.revokeObjectURL(workletURL);
        }

        recorderNode?.disconnect();
        recorderNode = new AudioWorkletNode(audioContext, "recorder-processor");

        // receive Float32Array chunks from worklet
        recorderNode.port.onmessage = (event) => {
          const floatSamples = event.data; // still owned by worklet
          writeToCircularBuffer(floatSamples);
          updateLevelMeter(floatSamples);
        };
        // Connect mic â†’ worklet â†’ destination (optional for monitoring)
        sourceNode.connect(recorderNode);
        // recorderNode.connect(audioContext.destination); // uncomment to hear yourself
        // connect a gain of 0 to destination if some browsers require a connection to destination
        // but we avoid audible output by not connecting workletNode to destination.
        recStatus.textContent = 'Status: recording';
        recording = true;

        log("ðŸŽ¬ Recording started with:", chosenDevice.label);
      }

      function stopRecording() {
        log('stopRecording');
        if (micStream) {
          micStream.getTracks().forEach(t => t.stop());
          micStream = null;
        }
        if (sourceNode) {
          try { sourceNode.disconnect(); } catch (e) {
            log('sourceNode disconnect error', e);
          }
          sourceNode = null;
        }
        recStatus.textContent = 'Status: stopped';
        recording = false;
      }

      function writeToCircularBuffer(floatSamples) {
        // floatSamples is a Float32Array
        for (let i = 0; i < floatSamples.length; i++) {
          const s = Math.max(-1, Math.min(1, floatSamples[i]));
          const int16 = s < 0 ? s * 0x8000 : s * 0x7FFF;
          buffer[writeIndex] = Math.round(int16);
          writeIndex++;
          if (writeIndex >= bufferLength) writeIndex = 0;
        }
      }

      function updateLevelMeter(floatSamples) {
        let sum = 0;
        for (let i = 0; i < floatSamples.length; i++) sum += floatSamples[i] * floatSamples[i];
        const rms = Math.sqrt(sum / floatSamples.length || 0);
        currentVolume = rms;
        levelBar.style.width = Math.min(1, rms * 4) * 100 + '%';
      }

      // ---------- Snippet extraction ----------
      function extractLastNSeconds(seconds) {
        log('extractLastNSeconds');
        if (!buffer) return new Int16Array(0);

        log('extractLastNSeconds sampleRate', sampleRate);
        const samplesNeeded = Math.min(Math.floor(sampleRate * seconds), bufferLength);
        const result = new Int16Array(samplesNeeded);
        let start = writeIndex - samplesNeeded;
        if (start < 0) start += bufferLength;
        if (start + samplesNeeded <= bufferLength) {
          result.set(buffer.subarray(start, start + samplesNeeded), 0);
        } else {
          const firstPart = bufferLength - start;
          result.set(buffer.subarray(start, start + firstPart), 0);
          result.set(buffer.subarray(0, samplesNeeded - firstPart), firstPart);
        }
        return result;
      }

      function extractLastNSecondsForLive(seconds) {
        //log('extractLastNSecondsForLive');
        if (!buffer) return new Int16Array(0);
        const samplesNeeded = Math.min(Math.floor(sampleRate * seconds), bufferLength);
        // reuse an Int16Array pool (store raw int16 PCM)
        if (!reuseBuffer) reuseBuffer = new Int16Array(LIVE_SECONDS * sampleRate);
        const result = reuseBuffer;
        const total = Math.min(samplesNeeded, result.length);
        let start = writeIndex - total;
        if (start < 0) start += bufferLength;
        if (start + total <= bufferLength) {
          // contiguous
          result.set(buffer.subarray(start, start + total), 0);
        } else {
          // wrapped
          const firstPart = bufferLength - start;
          result.set(buffer.subarray(start, start + firstPart), 0);
          result.set(buffer.subarray(0, total - firstPart), firstPart);
        }
        return result.subarray(0, total);
      }

      function makeWavBlob(intSamples) {
        const numChannels = 1;
        const sampleRateLocal = sampleRate;
        const bufferLen = intSamples.length * 2; // 2 bytes per sample
        const wavBuffer = new ArrayBuffer(44 + bufferLen);
        const view = new DataView(wavBuffer);

        function writeString(dataview, offset, str) {
          for (let i = 0; i < str.length; i++) dataview.setUint8(offset + i, str.charCodeAt(i));
        }

        // Write WAV header
        writeString(view, 0, 'RIFF');
        view.setUint32(4, 36 + bufferLen, true);
        writeString(view, 8, 'WAVE');
        writeString(view, 12, 'fmt ');
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRateLocal, true);
        view.setUint32(28, sampleRateLocal * numChannels * 2, true);
        view.setUint16(32, numChannels * 2, true);
        view.setUint16(34, 16, true);
        writeString(view, 36, 'data');
        view.setUint32(40, bufferLen, true);

        // Write PCM data
        let offset = 44;
        for (let i = 0; i < intSamples.length; i++, offset += 2) {
          view.setInt16(offset, intSamples[i], true);
        }

        return new Blob([view], { type: 'audio/wav' });
      }

      async function createSnippet(seconds) {
        log('createSnippet');
        recStatus.textContent = 'Status: creating snippetâ€¦';
        await ensureAudioContext();

        // how many seconds have been recorded? buffer may not be fully filled yet
        // compute recorded samples: if writeIndex wrapped, bufferLength else writeIndex
        const recordedSamples = Math.min(bufferLength, writeIndex || bufferLength);
        const totalRecordedSec = recordedSamples / sampleRate;
        const actualSeconds = Math.min(seconds, totalRecordedSec);
        if (actualSeconds <= 0) {
          alert('No audio available yet. Wait a few seconds and try again.');
          recStatus.textContent = 'Status: recording';
          return;
        }

        const intSamples = extractLastNSeconds(actualSeconds);
        const blob = makeWavBlob(intSamples);
        const url = URL.createObjectURL(blob);
        const snippetTime = new Date();

        if (snippetsArea.children.length == 1) document.getElementById('no-snippet-msg').style.display = 'none';

        // keep max 10 visible snippets
        while (snippetsArea.children.length >= 10) {
          snippetsArea.removeChild(snippetsArea.lastChild);
        }

        const card = document.createElement('div');
        card.className = 'snippet-card';
        const header = document.createElement('div');
        header.style.display = 'flex';
        header.style.justifyContent = 'space-between';
        header.style.alignItems = 'center';

        const titleDiv = document.createElement('div');
        titleDiv.style.fontWeight = '700';
        titleDiv.textContent = `${formatSeconds(actualSeconds)} snippet`;
        header.appendChild(titleDiv);

        const dateDiv = document.createElement('div');
        dateDiv.style.display = 'flex';
        dateDiv.style.alignItems = 'center';
        dateDiv.style.gap = '6px';

        const dateText = document.createElement('span');
        dateText.className = 'muted small';
        dateText.textContent = snippetTime.toLocaleString();
        dateDiv.appendChild(dateText);

        const removeBtn = document.createElement('button');
        removeBtn.textContent = 'Ã—';
        removeBtn.style.background = 'transparent';
        removeBtn.style.border = 'none';
        removeBtn.style.color = 'var(--danger)';
        removeBtn.style.cursor = 'pointer';
        removeBtn.style.fontSize = '14px';
        removeBtn.onclick = () => {
          snippetsArea.removeChild(card);
          if (snippetsArea.children.length === 1) document.getElementById('no-snippet-msg').style.display = 'block';
        };
        dateDiv.appendChild(removeBtn);

        header.appendChild(dateDiv);
        card.appendChild(header);


        // waveform canvas (HiDPI)
        const c = document.createElement('canvas');
        c.style.width = '100%';
        c.style.height = '120px';
        const dpr = window.devicePixelRatio || 1;
        const parentWidth = Math.max(800, snippetsArea.clientWidth || 800);
        c.width = Math.round(parentWidth * dpr);
        c.height = Math.round(120 * dpr);
        card.appendChild(c);
        drawSnippetWaveform(intSamples, c);

        // audio controls + download
        const controls = document.createElement('div');
        controls.className = 'snippet-controls';
        const playerWrap = document.createElement('div');
        playerWrap.className = 'player';

        const audio = document.createElement('audio');
        audio.controls = true;
        audio.src = url;
        audio.preload = 'auto';
        playerWrap.appendChild(audio);

        const dl = document.createElement('button');
        dl.className = 'download';
        dl.textContent = 'Download';
        dl.onclick = () => {
          const stamp = snippetTime;
          const timeStr = stamp.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit', second: '2-digit', hour12: true })
            .replaceAll(':', '-').replace(/\s/g, '');
          const dateStr = stamp.toLocaleDateString([], { month: 'numeric', day: 'numeric', year: 'numeric' })
            .replaceAll('/', '-');
          const name = `${formatSeconds(actualSeconds)} ${timeStr} ${dateStr} audio-timemachine.wav`;
          const a = document.createElement('a');
          a.href = url;
          a.download = name;
          a.click();
        };

        controls.appendChild(playerWrap);
        controls.appendChild(dl);
        card.appendChild(controls);
        snippetsArea.prepend(card);

        // track players and ensure only one plays
        allPlayers.push(audio);
        if (allPlayers.length > 10) allPlayers.shift();

        audio.addEventListener('play', () => {
          allPlayers.forEach(p => { if (p !== audio) p.pause(); });
          lastClickedSnippet = audio;
        });
        audio.addEventListener('click', () => { lastClickedSnippet = audio; });

        c.addEventListener('click', () => {
          lastClickedSnippet = audio;
          audio.play();
        });

        playersetSeekUI(audio, c, intSamples);

        recStatus.textContent = 'Status: recording';
      }

      // show playback progress overlay on snippet waveform
      function playersetSeekUI(audio, canvasEl, intSamples) {
        const ctx = canvasEl.getContext('2d');
        const w = canvasEl.width;
        const h = canvasEl.height;
        function drawProgress() {
          drawSnippetWaveform(intSamples, canvasEl);
          const pct = audio.duration ? (audio.currentTime / audio.duration) : 0;
          ctx.fillStyle = 'rgba(100,255,200,0.08)';
          ctx.fillRect(0, 0, Math.max(2, pct * w), h);
          ctx.fillStyle = 'rgba(255,255,255,0.08)';
          ctx.fillRect(Math.max(0, pct * w - 1), 0, 2, h);
        }
        audio.addEventListener('timeupdate', drawProgress);
        canvasEl.addEventListener('click', (ev) => {
          const rect = canvasEl.getBoundingClientRect();
          const x = ev.clientX - rect.left;
          let percent = x / rect.width;
          if (percent <= 0.03) percent = 0;
          if (audio.duration) audio.currentTime = percent * audio.duration;
          drawProgress();
        });
      }

      // waveform drawing for canvas (handles HiDPI canvas sizes)
      function drawSnippetWaveform(intSamples, canvasEl) {
        const ctx = canvasEl.getContext('2d');
        const w = canvasEl.width;
        const h = canvasEl.height;

        // Clear the canvas
        ctx.clearRect(0, 0, w, h);

        // Background gradient
        const g = ctx.createLinearGradient(0, 0, 0, h);
        g.addColorStop(0, 'rgba(255,255,255,0.01)');
        g.addColorStop(1, 'rgba(255,255,255,0.00)');
        ctx.fillStyle = g;
        ctx.fillRect(0, 0, w, h);

        // Draw the waveform
        ctx.lineWidth = 1;
        ctx.strokeStyle = 'rgba(110,231,183,0.95)';
        ctx.beginPath();

        const pixelWidth = w; // Number of vertical lines to draw
        const step = Math.max(1, Math.ceil(intSamples.length / pixelWidth)); // Samples per pixel column

        for (let i = 0; i < pixelWidth; i++) {
          const start = i * step;
          let min = 32767, max = -32768; // Int16 range

          // Find min and max values for the current column
          for (let j = 0; j < step && (start + j) < intSamples.length; j++) {
            const sample = intSamples[start + j];
            if (sample < min) min = sample;
            if (sample > max) max = sample;
          }

          // Scale min and max to canvas height
          const y1 = ((1 - (min / 32768)) / 2) * h;
          const y2 = ((1 - (max / 32768)) / 2) * h;

          // Draw the line for this column
          ctx.moveTo(i, y1);
          ctx.lineTo(i, y2);
        }

        ctx.stroke();
      }

      // live waveform draw loop
      let lastDraw = 0;
      function drawLiveWave() {
        const now = performance.now();
        if (now - lastDraw < 200) { requestAnimationFrame(drawLiveWave); return; }
        lastDraw = now;

        const canvas = liveWave;
        const ctx = liveCtx;
        const dpr = devicePixelRatio || 1;
        const wCss = canvas.clientWidth;
        const hCss = canvas.clientHeight;
        const w = Math.round(wCss * dpr);
        const h = Math.round(hCss * dpr);
        if (canvas.width !== w || canvas.height !== h) {
          canvas.width = w;
          canvas.height = h;
        }

        ctx.clearRect(0, 0, w, h);
        ctx.save();
        ctx.scale(dpr, dpr);
        ctx.lineWidth = 1.2;
        ctx.strokeStyle = 'rgba(110,231,183,0.95)';
        ctx.beginPath();

        // extract returns Int16Array now (raw PCM). compute min/max in int domain then scale once.
        const samples = extractLastNSecondsForLive(LIVE_SECONDS);
        if (samples.length > 0) {
          const step = Math.max(1, Math.floor(samples.length / wCss));
          for (let x = 0; x < wCss; x++) {
            const start = x * step;
            let minInt = 32767, maxInt = -32768;
            for (let k = 0; k < step && (start + k) < samples.length; k++) {
              const vInt = samples[start + k];
              if (vInt < minInt) minInt = vInt;
              if (vInt > maxInt) maxInt = vInt;
            }
            // scale to [-1,1]
            const min = minInt / 32768;
            const max = maxInt / 32768;
            const y1 = (1 - (min + 1) / 2) * hCss;
            const y2 = (1 - (max + 1) / 2) * hCss;
            ctx.moveTo(x, y1);
            ctx.lineTo(x, y2);
          }
          ctx.stroke();
        } else {
          ctx.fillStyle = 'rgba(255,255,255,0.03)';
          ctx.font = '12px system-ui';
          ctx.fillText('waiting for audioâ€¦', 12, 18);
        }

        ctx.restore();
        requestAnimationFrame(drawLiveWave);
      }

      function formatSeconds(s) {
        s = Math.trunc(s);
        const h = Math.floor(s / 3600);
        const m = Math.floor((s % 3600) / 60);
        const sec = s % 60;

        if (h > 0) {
          return `${h}h${m ? m + 'm' : ''}${sec ? sec + 's' : ''}`;
        } else if (m > 0) {
          return `${m}m${sec ? sec + 's' : ''}`;
        } else {
          return `${sec}s`;
        }
      }

      // cleanup on page unload
      window.addEventListener('unload', () => { if (micStream) micStream.getTracks().forEach(t => t.stop()); });

      // spacebar toggles play/pause for last clicked snippet
      window.addEventListener('keydown', ev => {
        if (ev.code === 'Space') {
          ev.preventDefault();

          if (lastClickedSnippet) {
            // toggle play/pause for the last clicked snippet
            if (lastClickedSnippet.paused) lastClickedSnippet.play();
            else lastClickedSnippet.pause();
          } else {
            // fallback: play the first snippet if available
            const firstAudio = snippetsArea.querySelector('audio');
            if (firstAudio) {
              firstAudio.play();
              lastClickedSnippet = firstAudio;
            }
            // if no audio exists, do nothing
          }
        }
      });

      window.addEventListener('visibilitychange', () => {
        if (document.hidden) {
          logToDiv('Went Background. AudioContext: ', audioContext?.state);
        } else {
          logToDiv('Came Foreground. AudioContext: ', audioContext?.state);
          audioContext?.resume();
        }
      });

      function log(...msg) {
        console.log(...msg);
      }

      function logToDiv(...msg) {
        log(...msg);
        const timestamp = new Date().toLocaleTimeString();
const message = msg.map(m => (typeof m === 'string' ? m : JSON.stringify(m))).join(' ');

        const line = document.createElement('div');
        line.textContent = `[${timestamp}] ${message}`;
        loggerDiv.appendChild(line);
      }


    })();
  </script>
</body>

</html>
